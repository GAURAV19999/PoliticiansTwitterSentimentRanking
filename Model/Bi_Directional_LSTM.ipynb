{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fluid-executive",
   "metadata": {},
   "source": [
    "# Bi Directional Model for classification of tweet sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-terminal",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-pasta",
   "metadata": {},
   "source": [
    "# 1. Installs and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-parcel",
   "metadata": {},
   "source": [
    "## 1.1. Install all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stuffed-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to install all required libraries\n",
    "# !pip3 install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-chemistry",
   "metadata": {},
   "source": [
    "## 1.2. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "induced-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.initializers import constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-smell",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-version",
   "metadata": {},
   "source": [
    "# 2. Load cleaned tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-voltage",
   "metadata": {},
   "source": [
    "# 3. Drop text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sentiment', 'Snowball_Stem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-harassment",
   "metadata": {},
   "source": [
    "# 4. Drop rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-beach",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-carol",
   "metadata": {},
   "source": [
    "# 5. Split dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Snowball_Stem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-beast",
   "metadata": {},
   "source": [
    "- 80% training data\n",
    "- 20% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-deployment",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-holly",
   "metadata": {},
   "source": [
    "# 6. Collection of all unique words in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of all unique words\n",
    "\n",
    "def count_unique_words(tweets):\n",
    "    unique = Counter()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet.split():\n",
    "            unique[word] += 1\n",
    "    return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = count_unique_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-conditioning",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-marshall",
   "metadata": {},
   "source": [
    "# 7. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-panel",
   "metadata": {},
   "source": [
    "## 7.1. Max number of words in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specialized-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-parameter",
   "metadata": {},
   "source": [
    "## 7.2. Create / Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SavedModels/BLSTM_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-opinion",
   "metadata": {},
   "source": [
    "## 7.3. Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for each word in tokenizer\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-azerbaijan",
   "metadata": {},
   "source": [
    "## 7.4. Convert training data to tokenized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-action",
   "metadata": {},
   "source": [
    "## 7.5. Padding training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-publication",
   "metadata": {},
   "source": [
    "## 7.6. Performing tokenization and padding for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-submission",
   "metadata": {},
   "source": [
    "## 7.7. Understanding training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-signal",
   "metadata": {},
   "source": [
    "## 7.8 Training the model (Custom embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Maps each word to a finite vector\n",
    "\n",
    "model.add(Embedding(len(word_index) + 1, 20, input_length=max_seq_length))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(64, dropout = 0.2, return_sequences = True)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, dropout = 0.2)))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pad, y_train, epochs=1, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./SavedModels/B_LSTM_train_74_12_val_77_79_test_?_acc.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-circumstances",
   "metadata": {},
   "source": [
    "# 7.9. Training the model (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('./SavedModels/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Maps each word to a finite vector\n",
    "\n",
    "model.add(Embedding(len(word_index)+1, 100, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(64, dropout = 0.2, return_sequences = True)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, dropout = 0.2)))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pad, y_train, epochs=3, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./SavedModels/B_LSTM_GloVe_train_73_59_val_74_82_test_74_34_acc_epoch_4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-shell",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-peter",
   "metadata": {},
   "source": [
    "# 8. Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-disco",
   "metadata": {},
   "source": [
    "## 8.1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('./SavedModels/B_LSTM_train_76_26_val_77_86_test_78_57_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indian-greensboro",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./SavedModels/B_LSTM_GloVe_train_73_59_val_74_82_test_74_34_acc_epoch_4.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88b534697159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./SavedModels/B_LSTM_GloVe_train_73_59_val_74_82_test_74_34_acc_epoch_4.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./SavedModels/B_LSTM_GloVe_train_73_59_val_74_82_test_74_34_acc_epoch_4.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./SavedModels/B_LSTM_GloVe_train_73_59_val_74_82_test_74_34_acc_epoch_4.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-priest",
   "metadata": {},
   "source": [
    "## 8.2. Create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.concat([test_df[test_df.sentiment != 0][:100000], test_df[test_df.sentiment == 0][:100000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['sentiment', 'Snowball_Stem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-willow",
   "metadata": {},
   "source": [
    "## 8.3. Drop rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-chemical",
   "metadata": {},
   "source": [
    "## 8.4. Tokenization and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = test_df['Snowball_Stem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_seq = tokenizer.texts_to_sequences(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_pad = pad_sequences(test_tweet_seq, maxlen=max_seq_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_pad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-matter",
   "metadata": {},
   "source": [
    "## 8.5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_tweet_pad, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss on test set:\", loss)\n",
    "print(\"Accuracy achieve on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-visibility",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-dodge",
   "metadata": {},
   "source": [
    "# 9. Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./SavedModels/B_LSTM_train_76_26_val_77_86_test_78_57_acc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SavedModels/LSTM_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
